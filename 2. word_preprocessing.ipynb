{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing libraries you need to install\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Import yfinance and pandas_datareader\n",
    "from pandas_datareader import data as pdr\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "# import yfinance as yf \n",
    "\n",
    "# Override function to store data we get\n",
    "# yf.pdr_override()\n",
    "\n",
    "# Import nltk for first step extracting words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Set up stop_words from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= {'10-k', 'form', 'table', 'contents', 'united', 'states', 'securities', 'exchange', 'commission'}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\"\"\"\n",
    "this is where different from version 1\n",
    "\"\"\"\n",
    "#import libraries for n-gram counting\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from utils.crawler import remove_punct, is_words"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/luckywang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/luckywang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/luckywang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## now we have all the txts stored in the file:\n",
    "'./data/10k/[cik]/rawtext/[cik]_[date]'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## we can make a dictionary to store all the data needed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# read the ticker library of all the tikers into ticker_library\n",
    "ticker_library = pd.read_csv(os.path.join(\"data\", \"tickers.csv\"))\n",
    "\n",
    "# read the sp500 components into ticker_selected, 'name' is the company name and ticker is company's ticker\n",
    "ticker_selected = pd.read_csv(os.path.join(\"data\", \"SP500_component_stocks.csv\"), header = None)\n",
    "ticker_selected.columns = ['name','ticker']\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/luckywang/Documents/Document/Course Material/Fall 2021/esg_nlp/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# build a ticker_cik_df dataframe to store ticker and its cik number\n",
    "ticker_cik_df = pd.DataFrame()\n",
    "\n",
    "# store all the tickers in a ticker_list\n",
    "ticker_list = ticker_selected.ticker\n",
    "\n",
    "# build a list cik_list for cik\n",
    "cik_list = []\n",
    "\n",
    "for ticker in ticker_list:    \n",
    "    try:\n",
    "        # for a given ticker, find its cik number through th ticker library\n",
    "        cik_list.append(list(ticker_library[ticker_library.ticker == ticker].secfilings)[0][-10:])\n",
    "        \n",
    "    except:\n",
    "        # if could not find cik, give it a empty cik\n",
    "        cik_list.append('')\n",
    "\n",
    "# write cik_list and ticker_list to the dataframe ticker_cik_df\n",
    "ticker_cik_df['cik'] = cik_list\n",
    "ticker_cik_df['ticker'] = ticker_list\n",
    "\n",
    "# delete the tickers with empty cik number\n",
    "ticker_cik_df = ticker_cik_df[ticker_cik_df['cik'] != '']\n",
    "\n",
    "# display a sample of ticker_cik_df\n",
    "ticker_cik_df.head()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001090872</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000006201</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001158449</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000320193</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001551152</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cik ticker\n",
       "0  0001090872      A\n",
       "1  0000006201    AAL\n",
       "2  0001158449    AAP\n",
       "3  0000320193   AAPL\n",
       "4  0001551152   ABBV"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "CIK2TICKER = {row[\"cik\"]: row[\"ticker\"] for _, row in ticker_cik_df.iterrows()}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "listtickers = ['AMZN','BBY','BKNG','MCD','EBAY','F','HD','TGT','WHR','JPM','SIVB','CFG','C','ALL','IVZ','ETFC','MET','PFG','CBOE',\n",
    "              'CTL','IPG','VIAC','NFLX','CHTR','FB','TWTR','NWSA','FOXA','AMD','INTC','AAPL','LRCX','MSFT','NLOK','CTSH','ADS',\n",
    "              'WU','PAYC','ABT','CVS','PFE','JNJ','BIIB','INCY','HSIC','WAT','ALGN','EW']\n",
    "\n",
    "ticker_cik_sample = pd.DataFrame()\n",
    "\n",
    "for ticker in listtickers:\n",
    "    ticker_cik_sample = ticker_cik_sample.append(ticker_cik_df[ticker_cik_df['ticker'] == ticker])\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "cik_list = ticker_cik_sample[\"cik\"].values\n",
    "ticker_list = ticker_cik_sample[\"ticker\"].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# store data using dictionary\n",
    "all_data = {}\n",
    "\n",
    "# set the key of dictionary as ticker\n",
    "for cik, ticker in zip(cik_list, ticker_list):\n",
    "    \n",
    "    # set the value of tikcer as a dict\n",
    "    all_data[ticker] = {}\n",
    "\n",
    "    # set the dict data[ticker] \n",
    "    all_data[ticker]['cik'] = cik\n",
    "    all_data[ticker]['10ks'] = {}\n",
    "    all_data[ticker]['10qs'] = {}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "all_data['AMZN']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'cik': '0001018724', '10ks': {}, '10qs': {}}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "dir_10k = './data/10k/'\n",
    "dir_10q = './data/10q/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(len(cik_list))\n",
    "print(len(ticker_list))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "45\n",
      "45\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "def aggregate_cik_texts(cik, filetype):\n",
    "    \"\"\"\n",
    "    Collect all the texts related to given `cik` with given filetype and \n",
    "    return a single string which concatenate all docs\n",
    "    \"\"\"\n",
    "    cik_dir = os.path.join(\"data\", filetype, cik)\n",
    "    pkl_path = os.path.join(cik_dir, \"pickle\")\n",
    "\n",
    "    if not os.path.isdir(pkl_path):\n",
    "        os.mkdir(pkl_path)\n",
    "    else:\n",
    "        # If already processed before, directly read the cache and return\n",
    "        with open(os.path.join(pkl_path, 'agg_texts.pkl'), 'rb') as f:\n",
    "            texts = pickle.load(f)\n",
    "        with open(os.path.join(pkl_path, 'token_counter.pkl'), 'rb') as f:\n",
    "            counter = pickle.load(f)\n",
    "        return {\"texts\": texts, \"counter\": counter}\n",
    "\n",
    "    rawtext_dir = os.path.join(cik_dir, \"rawtext\")\n",
    "    # goes into the directory to find the path for txtfiles\n",
    "    try:\n",
    "        all_files = os.listdir(rawtext_dir)\n",
    "    except:\n",
    "        print(\"No such dir\")\n",
    "    \n",
    "    texts = \"\"\n",
    "    for file in all_files:\n",
    "        with open(os.path.join(\"data\", filetype, cik, \"rawtext\", file), encoding = \"utf8\") as f:\n",
    "            string_temp = f.read().lower()\n",
    "            texts += preprocess(string_temp)\n",
    "    \n",
    "    texts = remove_punct(texts)\n",
    "    counter = texts2counter(texts)\n",
    "\n",
    "    with open(os.path.join(pkl_path, 'agg_texts.pkl'), 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        print(os.path.join(pkl_path, 'agg_texts.pkl'))\n",
    "        pickle.dump(texts, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    with open(os.path.join(pkl_path, 'token_counter.pkl'), 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        print(os.path.join(pkl_path, 'token_counter.pkl'))\n",
    "        pickle.dump(counter, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return {\"texts\": texts, \"counter\": counter}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def preprocess(texts):\n",
    "    \"\"\" \n",
    "    Tokenize texts, remove stopwords and numbers, and keep only the relevant words,\n",
    "    then lemmatize the tokens\n",
    "    \"\"\"\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "#     for w in words:\n",
    "#         rootWord=ps.stem(w)\n",
    "    \n",
    "#     tokens = [lemmatizer.lemmatize(token) for token in nltk.word_tokenize(texts) if token not in stop_words and is_words(token)]\n",
    "    tokens = [ps.stem(token) for token in nltk.word_tokenize(texts) if token not in stop_words and is_words(token)]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def texts2counter(texts):\n",
    "    tokens = texts.split(' ')\n",
    "    counter = Counter(tokens)\n",
    "    \n",
    "    return counter\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "now = datetime.now() # current date and time\n",
    "\n",
    "date_time = now.strftime(\"%m-%d-%H_%M_%S\")\n",
    "print(\"date and time:\",date_time)\t"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "date and time: 09-29-15_32_58\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "def get_texts(cik_list, ticker_list):\n",
    "    # './data/10k/[cik]/rawtext/[cik]_[date]'\n",
    "    docs = []\n",
    "    tickers = []\n",
    "    counters = dict()   # {ticker: counter}\n",
    "\n",
    "    for cik, ticker in tqdm(zip(cik_list, ticker_list)):\n",
    "        tickers.append(ticker)\n",
    "        texts = \"\"\n",
    "        for filetype in [\"10k\", \"10q\"]:\n",
    "            dict_ret = aggregate_cik_texts(cik, filetype)\n",
    "            texts += dict_ret[\"texts\"]\n",
    "        \n",
    "        counter = texts2counter(texts)\n",
    "        counters[ticker] = counter\n",
    "\n",
    "        docs.append(texts)\n",
    "    \n",
    "    date_time = now.strftime(\"%m-%d-%H_%M_%S\")\n",
    "    cache_path = os.path.join(\"data\", date_time)\n",
    "\n",
    "    if not os.path.exists(cache_path):\n",
    "        os.mkdir(cache_path)\n",
    "    \n",
    "    with open(os.path.join(cache_path, 'agg_counters.pkl'), 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(counters, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(os.path.join(cache_path, 'agg_texts.pkl'), 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(docs, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return {\"docs\": docs, \"tickers\": tickers, \"counters\": counters}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "ret = get_texts(cik_list[:2], ticker_list[:2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  2.87it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data/10k/0000764478/pickle/agg_texts.pkl\n",
      "data/10k/0000764478/pickle/token_counter.pkl\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [00:43<00:00, 22.00s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data/10q/0000764478/pickle/agg_texts.pkl\n",
      "data/10q/0000764478/pickle/token_counter.pkl\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "cv = CountVectorizer(max_df=0.8, stop_words=stop_words, max_features=10000)\n",
    "word_count_vector = cv.fit_transform(docs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/luckywang/Documents/Document/Course Material/Fall 2021/esg_nlp/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['10'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results = {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]] = score_vals[idx]\n",
    "    \n",
    "    return results\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "# tf_idf_vector = tfidf_transformer.transform(word_count_vector)\n",
    "# tf_idf_vector = tfidf_transformer.transform(cv.transform([docs[3]]))\n",
    "# tf_idf_vector = tfidf_transformer.transform(cv.transform(docs[0]))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "len(docs)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "keywords = []\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    tf_idf_vector = tfidf_transformer.transform(cv.transform([docs[i]]))\n",
    "    # you only needs to do this once, this is a mapping of index to \n",
    "    feature_names = cv.get_feature_names()\n",
    "\n",
    "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "    # extract only the top n; n here is 10\n",
    "    keyword = extract_topn_from_vector(feature_names, sorted_items, 30)\n",
    "    \n",
    "    keywords.append(keyword)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "dict_top_k = defaultdict(list)\n",
    "dict_top_k[\"tickers\"] = tickers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "for keyword in keywords:\n",
    "    for i, word in enumerate(keyword.keys()):\n",
    "        dict_top_k[\"word_{}\".format(i)].append(word)\n",
    "        dict_top_k[\"tfidf_{}\".format(i)].append(keyword[word])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "len(keywords)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "df_top_k_word = pd.DataFrame(dict_top_k)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "df_top_k_word = df_top_k_word.set_index(\"tickers\")\n",
    "df_top_k_word.filter(regex='word*', axis=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_0</th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>word_5</th>\n",
       "      <th>word_6</th>\n",
       "      <th>word_7</th>\n",
       "      <th>word_8</th>\n",
       "      <th>word_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_20</th>\n",
       "      <th>word_21</th>\n",
       "      <th>word_22</th>\n",
       "      <th>word_23</th>\n",
       "      <th>word_24</th>\n",
       "      <th>word_25</th>\n",
       "      <th>word_26</th>\n",
       "      <th>word_27</th>\n",
       "      <th>word_28</th>\n",
       "      <th>word_29</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tickers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>amazoncom</td>\n",
       "      <td>peac</td>\n",
       "      <td>amazon</td>\n",
       "      <td>ship</td>\n",
       "      <td>seller</td>\n",
       "      <td>bezo</td>\n",
       "      <td>equitymethod</td>\n",
       "      <td>merchandis</td>\n",
       "      <td>ecommerc</td>\n",
       "      <td>aw</td>\n",
       "      <td>...</td>\n",
       "      <td>shipment</td>\n",
       "      <td>wwwamazond</td>\n",
       "      <td>wwwamazoncouk</td>\n",
       "      <td>unearn</td>\n",
       "      <td>wwwamazoncojp</td>\n",
       "      <td>pledg</td>\n",
       "      <td>jeffrey</td>\n",
       "      <td>longzon</td>\n",
       "      <td>wrongdo</td>\n",
       "      <td>kindl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBY</th>\n",
       "      <td>sga</td>\n",
       "      <td>musicland</td>\n",
       "      <td>merchandis</td>\n",
       "      <td>applianc</td>\n",
       "      <td>magnolia</td>\n",
       "      <td>squad</td>\n",
       "      <td>geek</td>\n",
       "      <td>shop</td>\n",
       "      <td>ar</td>\n",
       "      <td>ebitdar</td>\n",
       "      <td>...</td>\n",
       "      <td>tradenam</td>\n",
       "      <td>phone</td>\n",
       "      <td>remodel</td>\n",
       "      <td>notebook</td>\n",
       "      <td>speakeasi</td>\n",
       "      <td>entertain</td>\n",
       "      <td>televis</td>\n",
       "      <td>canadian</td>\n",
       "      <td>auctionr</td>\n",
       "      <td>auction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BKNG</th>\n",
       "      <td>hotel</td>\n",
       "      <td>pricelinecom</td>\n",
       "      <td>bookingcom</td>\n",
       "      <td>airlin</td>\n",
       "      <td>ticket</td>\n",
       "      <td>hotelscom</td>\n",
       "      <td>kayak</td>\n",
       "      <td>car</td>\n",
       "      <td>merchant</td>\n",
       "      <td>pricedisclos</td>\n",
       "      <td>...</td>\n",
       "      <td>expedia</td>\n",
       "      <td>rentalcarscom</td>\n",
       "      <td>braddock</td>\n",
       "      <td>schulman</td>\n",
       "      <td>pricelin</td>\n",
       "      <td>nca</td>\n",
       "      <td>ctrip</td>\n",
       "      <td>agodacom</td>\n",
       "      <td>googl</td>\n",
       "      <td>walker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCD</th>\n",
       "      <td>restaur</td>\n",
       "      <td>companyoper</td>\n",
       "      <td>mcdonald</td>\n",
       "      <td>franchise</td>\n",
       "      <td>franchis</td>\n",
       "      <td>apmea</td>\n",
       "      <td>systemwid</td>\n",
       "      <td>development</td>\n",
       "      <td>menu</td>\n",
       "      <td>refranchis</td>\n",
       "      <td>...</td>\n",
       "      <td>nm</td>\n",
       "      <td>japan</td>\n",
       "      <td>chicken</td>\n",
       "      <td>convent</td>\n",
       "      <td>nutrit</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>omnibu</td>\n",
       "      <td>commod</td>\n",
       "      <td>occup</td>\n",
       "      <td>eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EBAY</th>\n",
       "      <td>paypal</td>\n",
       "      <td>ebay</td>\n",
       "      <td>seller</td>\n",
       "      <td>skype</td>\n",
       "      <td>gsi</td>\n",
       "      <td>merchant</td>\n",
       "      <td>ticket</td>\n",
       "      <td>card</td>\n",
       "      <td>stubhub</td>\n",
       "      <td>rolex</td>\n",
       "      <td>...</td>\n",
       "      <td>counterfeit</td>\n",
       "      <td>processor</td>\n",
       "      <td>client</td>\n",
       "      <td>copyright</td>\n",
       "      <td>meritori</td>\n",
       "      <td>shop</td>\n",
       "      <td>butterfield</td>\n",
       "      <td>billpoint</td>\n",
       "      <td>launder</td>\n",
       "      <td>licensur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>ford</td>\n",
       "      <td>automot</td>\n",
       "      <td>motor</td>\n",
       "      <td>incomeloss</td>\n",
       "      <td>securit</td>\n",
       "      <td>volvo</td>\n",
       "      <td>mazda</td>\n",
       "      <td>dealer</td>\n",
       "      <td>truck</td>\n",
       "      <td>fce</td>\n",
       "      <td>...</td>\n",
       "      <td>veba</td>\n",
       "      <td>jaguar</td>\n",
       "      <td>pag</td>\n",
       "      <td>fuel</td>\n",
       "      <td>statementsnot</td>\n",
       "      <td>nonconsum</td>\n",
       "      <td>warranti</td>\n",
       "      <td>commod</td>\n",
       "      <td>assetback</td>\n",
       "      <td>ghg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HD</th>\n",
       "      <td>depot</td>\n",
       "      <td>merchandis</td>\n",
       "      <td>hd</td>\n",
       "      <td>sga</td>\n",
       "      <td>expo</td>\n",
       "      <td>card</td>\n",
       "      <td>lumber</td>\n",
       "      <td>assort</td>\n",
       "      <td>carol</td>\n",
       "      <td>omnibu</td>\n",
       "      <td>...</td>\n",
       "      <td>rdc</td>\n",
       "      <td>remodel</td>\n",
       "      <td>ferri</td>\n",
       "      <td>blake</td>\n",
       "      <td>menear</td>\n",
       "      <td>atlanta</td>\n",
       "      <td>mexico</td>\n",
       "      <td>kpmg</td>\n",
       "      <td>contentsth</td>\n",
       "      <td>floor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TGT</th>\n",
       "      <td>card</td>\n",
       "      <td>guest</td>\n",
       "      <td>comparablestor</td>\n",
       "      <td>redcard</td>\n",
       "      <td>merchandis</td>\n",
       "      <td>sga</td>\n",
       "      <td>ebit</td>\n",
       "      <td>scovann</td>\n",
       "      <td>remodel</td>\n",
       "      <td>jpmc</td>\n",
       "      <td>...</td>\n",
       "      <td>ep</td>\n",
       "      <td>supertarget</td>\n",
       "      <td>visa</td>\n",
       "      <td>trc</td>\n",
       "      <td>marshal</td>\n",
       "      <td>minnesota</td>\n",
       "      <td>periodend</td>\n",
       "      <td>dougla</td>\n",
       "      <td>gift</td>\n",
       "      <td>linkbas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHR</th>\n",
       "      <td>whirlpool</td>\n",
       "      <td>befiex</td>\n",
       "      <td>brazilian</td>\n",
       "      <td>indesit</td>\n",
       "      <td>maytag</td>\n",
       "      <td>embraco</td>\n",
       "      <td>applianc</td>\n",
       "      <td>sundri</td>\n",
       "      <td>compressor</td>\n",
       "      <td>pricemix</td>\n",
       "      <td>...</td>\n",
       "      <td>warranti</td>\n",
       "      <td>refriger</td>\n",
       "      <td>forwardsopt</td>\n",
       "      <td>oilrel</td>\n",
       "      <td>monet</td>\n",
       "      <td>alno</td>\n",
       "      <td>hotpoint</td>\n",
       "      <td>amana</td>\n",
       "      <td>kitchenaid</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JPM</th>\n",
       "      <td>jpmorgan</td>\n",
       "      <td>lendingrel</td>\n",
       "      <td>securit</td>\n",
       "      <td>card</td>\n",
       "      <td>chargeoff</td>\n",
       "      <td>var</td>\n",
       "      <td>msr</td>\n",
       "      <td>noninterest</td>\n",
       "      <td>pci</td>\n",
       "      <td>client</td>\n",
       "      <td>...</td>\n",
       "      <td>nm</td>\n",
       "      <td>lend</td>\n",
       "      <td>af</td>\n",
       "      <td>creditimpair</td>\n",
       "      <td>multisel</td>\n",
       "      <td>tier</td>\n",
       "      <td>spe</td>\n",
       "      <td>cb</td>\n",
       "      <td>heldforsal</td>\n",
       "      <td>conduit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIVB</th>\n",
       "      <td>svb</td>\n",
       "      <td>client</td>\n",
       "      <td>noninterest</td>\n",
       "      <td>silicon</td>\n",
       "      <td>contentssvb</td>\n",
       "      <td>valley</td>\n",
       "      <td>nonmarket</td>\n",
       "      <td>bancshar</td>\n",
       "      <td>unfund</td>\n",
       "      <td>equityventur</td>\n",
       "      <td>...</td>\n",
       "      <td>fte</td>\n",
       "      <td>riskweight</td>\n",
       "      <td>standbi</td>\n",
       "      <td>tier</td>\n",
       "      <td>riskrat</td>\n",
       "      <td>volcker</td>\n",
       "      <td>debentur</td>\n",
       "      <td>bancventur</td>\n",
       "      <td>ftp</td>\n",
       "      <td>mortgageback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CFG</th>\n",
       "      <td>incmanag</td>\n",
       "      <td>basel</td>\n",
       "      <td>frb</td>\n",
       "      <td>cbna</td>\n",
       "      <td>rb</td>\n",
       "      <td>alll</td>\n",
       "      <td>var</td>\n",
       "      <td>incnot</td>\n",
       "      <td>noninterest</td>\n",
       "      <td>tier</td>\n",
       "      <td>...</td>\n",
       "      <td>msr</td>\n",
       "      <td>noncor</td>\n",
       "      <td>lend</td>\n",
       "      <td>lcr</td>\n",
       "      <td>nonperform</td>\n",
       "      <td>nonaccru</td>\n",
       "      <td>occ</td>\n",
       "      <td>investmentpostmodif</td>\n",
       "      <td>contractspremodif</td>\n",
       "      <td>appetit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>citigroup</td>\n",
       "      <td>securit</td>\n",
       "      <td>card</td>\n",
       "      <td>basel</td>\n",
       "      <td>gcb</td>\n",
       "      <td>tier</td>\n",
       "      <td>vie</td>\n",
       "      <td>citicorp</td>\n",
       "      <td>cdo</td>\n",
       "      <td>conduit</td>\n",
       "      <td>...</td>\n",
       "      <td>cva</td>\n",
       "      <td>fx</td>\n",
       "      <td>tob</td>\n",
       "      <td>residenti</td>\n",
       "      <td>htm</td>\n",
       "      <td>af</td>\n",
       "      <td>var</td>\n",
       "      <td>prioryear</td>\n",
       "      <td>cgmhi</td>\n",
       "      <td>mortgageback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALL</th>\n",
       "      <td>allstat</td>\n",
       "      <td>reinsur</td>\n",
       "      <td>propertyli</td>\n",
       "      <td>annuiti</td>\n",
       "      <td>contracthold</td>\n",
       "      <td>reestim</td>\n",
       "      <td>dac</td>\n",
       "      <td>catastroph</td>\n",
       "      <td>aic</td>\n",
       "      <td>homeown</td>\n",
       "      <td>...</td>\n",
       "      <td>policyhold</td>\n",
       "      <td>pif</td>\n",
       "      <td>mortal</td>\n",
       "      <td>lifeconting</td>\n",
       "      <td>fhcf</td>\n",
       "      <td>lbl</td>\n",
       "      <td>casualti</td>\n",
       "      <td>cmb</td>\n",
       "      <td>creditriskconting</td>\n",
       "      <td>equityindex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IVZ</th>\n",
       "      <td>aum</td>\n",
       "      <td>invesco</td>\n",
       "      <td>clo</td>\n",
       "      <td>cip</td>\n",
       "      <td>client</td>\n",
       "      <td>uit</td>\n",
       "      <td>seed</td>\n",
       "      <td>csip</td>\n",
       "      <td>policyhold</td>\n",
       "      <td>passiv</td>\n",
       "      <td>...</td>\n",
       "      <td>domicil</td>\n",
       "      <td>sterl</td>\n",
       "      <td>unconsolid</td>\n",
       "      <td>fundoffund</td>\n",
       "      <td>subgroup</td>\n",
       "      <td>starr</td>\n",
       "      <td>lossesreinvest</td>\n",
       "      <td>amvescap</td>\n",
       "      <td>ep</td>\n",
       "      <td>canadian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ETFC</th>\n",
       "      <td>etrad</td>\n",
       "      <td>fourfamili</td>\n",
       "      <td>brokerag</td>\n",
       "      <td>ajaxo</td>\n",
       "      <td>tdr</td>\n",
       "      <td>mortgageback</td>\n",
       "      <td>interestearn</td>\n",
       "      <td>chargeoff</td>\n",
       "      <td>fhlb</td>\n",
       "      <td>cmo</td>\n",
       "      <td>...</td>\n",
       "      <td>sweep</td>\n",
       "      <td>lien</td>\n",
       "      <td>contentsetrad</td>\n",
       "      <td>nonperform</td>\n",
       "      <td>ltvcltv</td>\n",
       "      <td>riskweight</td>\n",
       "      <td>nonaccru</td>\n",
       "      <td>fdic</td>\n",
       "      <td>otti</td>\n",
       "      <td>etbh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MET</th>\n",
       "      <td>metlif</td>\n",
       "      <td>policyhold</td>\n",
       "      <td>reinsur</td>\n",
       "      <td>annuiti</td>\n",
       "      <td>mlic</td>\n",
       "      <td>dac</td>\n",
       "      <td>contentsmetlif</td>\n",
       "      <td>metropolitan</td>\n",
       "      <td>voba</td>\n",
       "      <td>asbesto</td>\n",
       "      <td>...</td>\n",
       "      <td>contracthold</td>\n",
       "      <td>agricultur</td>\n",
       "      <td>surplu</td>\n",
       "      <td>cede</td>\n",
       "      <td>auto</td>\n",
       "      <td>rmb</td>\n",
       "      <td>lend</td>\n",
       "      <td>unitlink</td>\n",
       "      <td>af</td>\n",
       "      <td>cse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFG</th>\n",
       "      <td>policyhold</td>\n",
       "      <td>annuiti</td>\n",
       "      <td>pfg</td>\n",
       "      <td>residenti</td>\n",
       "      <td>aum</td>\n",
       "      <td>bt</td>\n",
       "      <td>realizedunr</td>\n",
       "      <td>mortgageback</td>\n",
       "      <td>investmenttyp</td>\n",
       "      <td>dpac</td>\n",
       "      <td>...</td>\n",
       "      <td>iowa</td>\n",
       "      <td>swaption</td>\n",
       "      <td>contentsprincip</td>\n",
       "      <td>postretir</td>\n",
       "      <td>demutu</td>\n",
       "      <td>residentialfirst</td>\n",
       "      <td>mortal</td>\n",
       "      <td>cuprum</td>\n",
       "      <td>mortar</td>\n",
       "      <td>bifurc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBOE</th>\n",
       "      <td>cboe</td>\n",
       "      <td>bat</td>\n",
       "      <td>occ</td>\n",
       "      <td>opra</td>\n",
       "      <td>cfe</td>\n",
       "      <td>fx</td>\n",
       "      <td>euroccp</td>\n",
       "      <td>vix</td>\n",
       "      <td>contentscbo</td>\n",
       "      <td>etp</td>\n",
       "      <td>...</td>\n",
       "      <td>cftc</td>\n",
       "      <td>spx</td>\n",
       "      <td>exchangetrad</td>\n",
       "      <td>multiplylist</td>\n",
       "      <td>unrestrict</td>\n",
       "      <td>outcri</td>\n",
       "      <td>chicago</td>\n",
       "      <td>cbsx</td>\n",
       "      <td>sef</td>\n",
       "      <td>chix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTL</th>\n",
       "      <td>centurylink</td>\n",
       "      <td>qwest</td>\n",
       "      <td>embarq</td>\n",
       "      <td>centurytel</td>\n",
       "      <td>broadband</td>\n",
       "      <td>fcc</td>\n",
       "      <td>savvi</td>\n",
       "      <td>lec</td>\n",
       "      <td>wireless</td>\n",
       "      <td>usf</td>\n",
       "      <td>...</td>\n",
       "      <td>voip</td>\n",
       "      <td>verizon</td>\n",
       "      <td>video</td>\n",
       "      <td>cabl</td>\n",
       "      <td>facilitiesbas</td>\n",
       "      <td>rural</td>\n",
       "      <td>interst</td>\n",
       "      <td>usgaapoperatingsegmentsmemb</td>\n",
       "      <td>mpl</td>\n",
       "      <td>wholesal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IPG</th>\n",
       "      <td>interpubl</td>\n",
       "      <td>client</td>\n",
       "      <td>cmg</td>\n",
       "      <td>ipg</td>\n",
       "      <td>ian</td>\n",
       "      <td>motorsport</td>\n",
       "      <td>mccann</td>\n",
       "      <td>pip</td>\n",
       "      <td>continuedamount</td>\n",
       "      <td>dooner</td>\n",
       "      <td>...</td>\n",
       "      <td>fcb</td>\n",
       "      <td>reorganizationrel</td>\n",
       "      <td>uncommit</td>\n",
       "      <td>worldgroup</td>\n",
       "      <td>philipp</td>\n",
       "      <td>ebitda</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>mccannerickson</td>\n",
       "      <td>prioryear</td>\n",
       "      <td>carrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX</th>\n",
       "      <td>dvd</td>\n",
       "      <td>subscrib</td>\n",
       "      <td>stream</td>\n",
       "      <td>netflix</td>\n",
       "      <td>librari</td>\n",
       "      <td>subscript</td>\n",
       "      <td>membership</td>\n",
       "      <td>movi</td>\n",
       "      <td>studio</td>\n",
       "      <td>gato</td>\n",
       "      <td>...</td>\n",
       "      <td>mail</td>\n",
       "      <td>hast</td>\n",
       "      <td>video</td>\n",
       "      <td>reed</td>\n",
       "      <td>internetconnect</td>\n",
       "      <td>suboptim</td>\n",
       "      <td>vod</td>\n",
       "      <td>freetrial</td>\n",
       "      <td>mccarthi</td>\n",
       "      <td>latticebinomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHTR</th>\n",
       "      <td>cabl</td>\n",
       "      <td>cco</td>\n",
       "      <td>holdco</td>\n",
       "      <td>cch</td>\n",
       "      <td>twc</td>\n",
       "      <td>video</td>\n",
       "      <td>franchis</td>\n",
       "      <td>allen</td>\n",
       "      <td>vulcan</td>\n",
       "      <td>cc</td>\n",
       "      <td>...</td>\n",
       "      <td>bright</td>\n",
       "      <td>subsidiariesnot</td>\n",
       "      <td>viii</td>\n",
       "      <td>pole</td>\n",
       "      <td>comcast</td>\n",
       "      <td>televis</td>\n",
       "      <td>cchc</td>\n",
       "      <td>settop</td>\n",
       "      <td>programm</td>\n",
       "      <td>cii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FB</th>\n",
       "      <td>facebook</td>\n",
       "      <td>mau</td>\n",
       "      <td>dau</td>\n",
       "      <td>zuckerberg</td>\n",
       "      <td>arpu</td>\n",
       "      <td>whatsapp</td>\n",
       "      <td>oculu</td>\n",
       "      <td>instagram</td>\n",
       "      <td>monet</td>\n",
       "      <td>messeng</td>\n",
       "      <td>...</td>\n",
       "      <td>dap</td>\n",
       "      <td>unproven</td>\n",
       "      <td>wehner</td>\n",
       "      <td>younger</td>\n",
       "      <td>nasdaq</td>\n",
       "      <td>inccondens</td>\n",
       "      <td>bug</td>\n",
       "      <td>server</td>\n",
       "      <td>app</td>\n",
       "      <td>inaccuraci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWTR</th>\n",
       "      <td>twitter</td>\n",
       "      <td>tweet</td>\n",
       "      <td>mau</td>\n",
       "      <td>mdau</td>\n",
       "      <td>spam</td>\n",
       "      <td>ipa</td>\n",
       "      <td>audienc</td>\n",
       "      <td>instagram</td>\n",
       "      <td>personnelrel</td>\n",
       "      <td>monet</td>\n",
       "      <td>...</td>\n",
       "      <td>similarlytitl</td>\n",
       "      <td>retweet</td>\n",
       "      <td>coloc</td>\n",
       "      <td>tellapart</td>\n",
       "      <td>traffic</td>\n",
       "      <td>googl</td>\n",
       "      <td>ebitda</td>\n",
       "      <td>inaccess</td>\n",
       "      <td>fake</td>\n",
       "      <td>server</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NWSA</th>\n",
       "      <td>fox</td>\n",
       "      <td>news</td>\n",
       "      <td>rea</td>\n",
       "      <td>foxtel</td>\n",
       "      <td>centuri</td>\n",
       "      <td>nam</td>\n",
       "      <td>newspap</td>\n",
       "      <td>harpercollin</td>\n",
       "      <td>valassi</td>\n",
       "      <td>australia</td>\n",
       "      <td>...</td>\n",
       "      <td>paytv</td>\n",
       "      <td>televis</td>\n",
       "      <td>betterwors</td>\n",
       "      <td>broadcast</td>\n",
       "      <td>newsprint</td>\n",
       "      <td>sunday</td>\n",
       "      <td>sun</td>\n",
       "      <td>harlequin</td>\n",
       "      <td>subscrib</td>\n",
       "      <td>ebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOXA</th>\n",
       "      <td>fox</td>\n",
       "      <td>sport</td>\n",
       "      <td>televis</td>\n",
       "      <td>broadcast</td>\n",
       "      <td>station</td>\n",
       "      <td>cabl</td>\n",
       "      <td>disney</td>\n",
       "      <td>foxtelevisionsegmentmemb</td>\n",
       "      <td>usgaapoperatingsegmentsmemb</td>\n",
       "      <td>ebitda</td>\n",
       "      <td>...</td>\n",
       "      <td>credibl</td>\n",
       "      <td>caffein</td>\n",
       "      <td>fcc</td>\n",
       "      <td>foxredeemablenoncontrollinginterestsmemb</td>\n",
       "      <td>lot</td>\n",
       "      <td>game</td>\n",
       "      <td>nfl</td>\n",
       "      <td>footbal</td>\n",
       "      <td>usgaapcommonstockmemb</td>\n",
       "      <td>subscrib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMD</th>\n",
       "      <td>amd</td>\n",
       "      <td>fab</td>\n",
       "      <td>gf</td>\n",
       "      <td>microprocessor</td>\n",
       "      <td>spansion</td>\n",
       "      <td>wafer</td>\n",
       "      <td>graphic</td>\n",
       "      <td>intel</td>\n",
       "      <td>chipset</td>\n",
       "      <td>saxoni</td>\n",
       "      <td>...</td>\n",
       "      <td>micro</td>\n",
       "      <td>processor</td>\n",
       "      <td>jv</td>\n",
       "      <td>fujitsu</td>\n",
       "      <td>semiconductor</td>\n",
       "      <td>wsa</td>\n",
       "      <td>atmp</td>\n",
       "      <td>flash</td>\n",
       "      <td>atic</td>\n",
       "      <td>amtc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTC</th>\n",
       "      <td>intel</td>\n",
       "      <td>microprocessor</td>\n",
       "      <td>chipset</td>\n",
       "      <td>processor</td>\n",
       "      <td>imft</td>\n",
       "      <td>memori</td>\n",
       "      <td>flash</td>\n",
       "      <td>dcg</td>\n",
       "      <td>nonmarket</td>\n",
       "      <td>mcafe</td>\n",
       "      <td>...</td>\n",
       "      <td>desktop</td>\n",
       "      <td>corporationnot</td>\n",
       "      <td>debentur</td>\n",
       "      <td>pentium</td>\n",
       "      <td>notebook</td>\n",
       "      <td>intergraph</td>\n",
       "      <td>semiconductor</td>\n",
       "      <td>wafer</td>\n",
       "      <td>motherboard</td>\n",
       "      <td>wireless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>ipod</td>\n",
       "      <td>appl</td>\n",
       "      <td>mac</td>\n",
       "      <td>iphon</td>\n",
       "      <td>macintosh</td>\n",
       "      <td>itun</td>\n",
       "      <td>os</td>\n",
       "      <td>music</td>\n",
       "      <td>ipad</td>\n",
       "      <td>warranti</td>\n",
       "      <td>...</td>\n",
       "      <td>microprocessor</td>\n",
       "      <td>app</td>\n",
       "      <td>nontrad</td>\n",
       "      <td>video</td>\n",
       "      <td>rsu</td>\n",
       "      <td>outsourc</td>\n",
       "      <td>cellular</td>\n",
       "      <td>carrier</td>\n",
       "      <td>powerbook</td>\n",
       "      <td>desktop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRCX</th>\n",
       "      <td>lam</td>\n",
       "      <td>semiconductor</td>\n",
       "      <td>etch</td>\n",
       "      <td>wafer</td>\n",
       "      <td>fremont</td>\n",
       "      <td>novellu</td>\n",
       "      <td>shipment</td>\n",
       "      <td>warranti</td>\n",
       "      <td>rd</td>\n",
       "      <td>clean</td>\n",
       "      <td>...</td>\n",
       "      <td>klatencor</td>\n",
       "      <td>outsourc</td>\n",
       "      <td>varian</td>\n",
       "      <td>bullen</td>\n",
       "      <td>tegal</td>\n",
       "      <td>newberri</td>\n",
       "      <td>usgaapforwardcontractsmemb</td>\n",
       "      <td>yendenomin</td>\n",
       "      <td>espp</td>\n",
       "      <td>pariba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>microsoft</td>\n",
       "      <td>window</td>\n",
       "      <td>xbox</td>\n",
       "      <td>server</td>\n",
       "      <td>oem</td>\n",
       "      <td>pc</td>\n",
       "      <td>headcountrel</td>\n",
       "      <td>consol</td>\n",
       "      <td>game</td>\n",
       "      <td>msn</td>\n",
       "      <td>...</td>\n",
       "      <td>voucher</td>\n",
       "      <td>entertain</td>\n",
       "      <td>subscript</td>\n",
       "      <td>search</td>\n",
       "      <td>datacent</td>\n",
       "      <td>surfac</td>\n",
       "      <td>intellig</td>\n",
       "      <td>commod</td>\n",
       "      <td>corporatelevel</td>\n",
       "      <td>skype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTSH</th>\n",
       "      <td>cogniz</td>\n",
       "      <td>indian</td>\n",
       "      <td>client</td>\n",
       "      <td>rupe</td>\n",
       "      <td>im</td>\n",
       "      <td>fixedpric</td>\n",
       "      <td>visa</td>\n",
       "      <td>offshor</td>\n",
       "      <td>stp</td>\n",
       "      <td>onsiteoffshor</td>\n",
       "      <td>...</td>\n",
       "      <td>coburn</td>\n",
       "      <td>wage</td>\n",
       "      <td>unbil</td>\n",
       "      <td>mat</td>\n",
       "      <td>export</td>\n",
       "      <td>dun</td>\n",
       "      <td>bradstreet</td>\n",
       "      <td>holiday</td>\n",
       "      <td>ub</td>\n",
       "      <td>scienc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADS</th>\n",
       "      <td>card</td>\n",
       "      <td>mile</td>\n",
       "      <td>wfn</td>\n",
       "      <td>securit</td>\n",
       "      <td>ebitda</td>\n",
       "      <td>comen</td>\n",
       "      <td>reward</td>\n",
       "      <td>epsilon</td>\n",
       "      <td>air</td>\n",
       "      <td>brandloyalti</td>\n",
       "      <td>...</td>\n",
       "      <td>merchant</td>\n",
       "      <td>databas</td>\n",
       "      <td>score</td>\n",
       "      <td>conduit</td>\n",
       "      <td>chargedoff</td>\n",
       "      <td>montreal</td>\n",
       "      <td>indexalli</td>\n",
       "      <td>aspen</td>\n",
       "      <td>breakag</td>\n",
       "      <td>wfcb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WU</th>\n",
       "      <td>consumertoconsum</td>\n",
       "      <td>companynot</td>\n",
       "      <td>spinoff</td>\n",
       "      <td>subag</td>\n",
       "      <td>speedpay</td>\n",
       "      <td>consumertobusi</td>\n",
       "      <td>wufsi</td>\n",
       "      <td>contentsth</td>\n",
       "      <td>crosscurr</td>\n",
       "      <td>westernunioncom</td>\n",
       "      <td>...</td>\n",
       "      <td>card</td>\n",
       "      <td>antimoney</td>\n",
       "      <td>continuedunauditedth</td>\n",
       "      <td>mexico</td>\n",
       "      <td>ersek</td>\n",
       "      <td>biller</td>\n",
       "      <td>crossbord</td>\n",
       "      <td>gainsloss</td>\n",
       "      <td>arizona</td>\n",
       "      <td>subpoena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABT</th>\n",
       "      <td>abbott</td>\n",
       "      <td>laboratori</td>\n",
       "      <td>pharmaceut</td>\n",
       "      <td>nutrit</td>\n",
       "      <td>tap</td>\n",
       "      <td>diagnost</td>\n",
       "      <td>vascular</td>\n",
       "      <td>abbvi</td>\n",
       "      <td>jude</td>\n",
       "      <td>aler</td>\n",
       "      <td>...</td>\n",
       "      <td>amo</td>\n",
       "      <td>solvay</td>\n",
       "      <td>fda</td>\n",
       "      <td>scientif</td>\n",
       "      <td>dental</td>\n",
       "      <td>xienc</td>\n",
       "      <td>freyman</td>\n",
       "      <td>cardiovascular</td>\n",
       "      <td>endovascular</td>\n",
       "      <td>cfr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CVS</th>\n",
       "      <td>pharmaci</td>\n",
       "      <td>caremark</td>\n",
       "      <td>cv</td>\n",
       "      <td>drug</td>\n",
       "      <td>pbm</td>\n",
       "      <td>prescript</td>\n",
       "      <td>medicar</td>\n",
       "      <td>retailltc</td>\n",
       "      <td>dispens</td>\n",
       "      <td>aetna</td>\n",
       "      <td>...</td>\n",
       "      <td>payor</td>\n",
       "      <td>formulari</td>\n",
       "      <td>cvspharmaci</td>\n",
       "      <td>pdp</td>\n",
       "      <td>pharmacist</td>\n",
       "      <td>merchandis</td>\n",
       "      <td>usgaapfairvaluemeasurementsrecurringmemb</td>\n",
       "      <td>qui</td>\n",
       "      <td>silverscript</td>\n",
       "      <td>linen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFE</th>\n",
       "      <td>pfizer</td>\n",
       "      <td>pharmaceut</td>\n",
       "      <td>fda</td>\n",
       "      <td>wyeth</td>\n",
       "      <td>lipitor</td>\n",
       "      <td>drug</td>\n",
       "      <td>pharmacia</td>\n",
       "      <td>medicin</td>\n",
       "      <td>patient</td>\n",
       "      <td>lyrica</td>\n",
       "      <td>...</td>\n",
       "      <td>biopharmaceut</td>\n",
       "      <td>pfebiopharmasegmentmemb</td>\n",
       "      <td>eh</td>\n",
       "      <td>rd</td>\n",
       "      <td>pain</td>\n",
       "      <td>mda</td>\n",
       "      <td>norvasc</td>\n",
       "      <td>vaccin</td>\n",
       "      <td>ih</td>\n",
       "      <td>disord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JNJ</th>\n",
       "      <td>johnson</td>\n",
       "      <td>janssen</td>\n",
       "      <td>pharmaceut</td>\n",
       "      <td>depuy</td>\n",
       "      <td>jnjpharmaceuticalmemb</td>\n",
       "      <td>cordi</td>\n",
       "      <td>jpi</td>\n",
       "      <td>anda</td>\n",
       "      <td>jbi</td>\n",
       "      <td>ethicon</td>\n",
       "      <td>...</td>\n",
       "      <td>diagnost</td>\n",
       "      <td>usgaapnonusmemb</td>\n",
       "      <td>lupin</td>\n",
       "      <td>biosimilar</td>\n",
       "      <td>actelion</td>\n",
       "      <td>asrtm</td>\n",
       "      <td>jersey</td>\n",
       "      <td>fda</td>\n",
       "      <td>lifescan</td>\n",
       "      <td>franchis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIIB</th>\n",
       "      <td>rituxan</td>\n",
       "      <td>tysabri</td>\n",
       "      <td>biogen</td>\n",
       "      <td>idec</td>\n",
       "      <td>avonex</td>\n",
       "      <td>genentech</td>\n",
       "      <td>collabor</td>\n",
       "      <td>clinic</td>\n",
       "      <td>zevalin</td>\n",
       "      <td>copromot</td>\n",
       "      <td>...</td>\n",
       "      <td>mileston</td>\n",
       "      <td>antibodi</td>\n",
       "      <td>samsung</td>\n",
       "      <td>fumapharm</td>\n",
       "      <td>biolog</td>\n",
       "      <td>therapi</td>\n",
       "      <td>neurimmun</td>\n",
       "      <td>therapeut</td>\n",
       "      <td>hemophilia</td>\n",
       "      <td>roch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INCY</th>\n",
       "      <td>drug</td>\n",
       "      <td>jakafi</td>\n",
       "      <td>clinic</td>\n",
       "      <td>collabor</td>\n",
       "      <td>incyt</td>\n",
       "      <td>fda</td>\n",
       "      <td>candid</td>\n",
       "      <td>ruxolitinib</td>\n",
       "      <td>patient</td>\n",
       "      <td>mileston</td>\n",
       "      <td>...</td>\n",
       "      <td>inhibitor</td>\n",
       "      <td>pfizer</td>\n",
       "      <td>calithera</td>\n",
       "      <td>myelofibrosi</td>\n",
       "      <td>arthriti</td>\n",
       "      <td>license</td>\n",
       "      <td>maxia</td>\n",
       "      <td>cancer</td>\n",
       "      <td>codevelop</td>\n",
       "      <td>gene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HSIC</th>\n",
       "      <td>schein</td>\n",
       "      <td>henri</td>\n",
       "      <td>dental</td>\n",
       "      <td>practition</td>\n",
       "      <td>anim</td>\n",
       "      <td>officebas</td>\n",
       "      <td>physician</td>\n",
       "      <td>pharmaceut</td>\n",
       "      <td>drug</td>\n",
       "      <td>valuead</td>\n",
       "      <td>...</td>\n",
       "      <td>fda</td>\n",
       "      <td>bergman</td>\n",
       "      <td>patterson</td>\n",
       "      <td>laboratori</td>\n",
       "      <td>benco</td>\n",
       "      <td>surgic</td>\n",
       "      <td>clinic</td>\n",
       "      <td>hsichealthcaredistributionmemb</td>\n",
       "      <td>vaccin</td>\n",
       "      <td>archer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WAT</th>\n",
       "      <td>water</td>\n",
       "      <td>ta</td>\n",
       "      <td>hplc</td>\n",
       "      <td>chromatographi</td>\n",
       "      <td>lc</td>\n",
       "      <td>thermal</td>\n",
       "      <td>spectrometri</td>\n",
       "      <td>uplc</td>\n",
       "      <td>acquiti</td>\n",
       "      <td>rheometri</td>\n",
       "      <td>...</td>\n",
       "      <td>uspensionplan</td>\n",
       "      <td>nonuspensionplan</td>\n",
       "      <td>micromass</td>\n",
       "      <td>yeartod</td>\n",
       "      <td>lcm</td>\n",
       "      <td>academ</td>\n",
       "      <td>quadrupol</td>\n",
       "      <td>pe</td>\n",
       "      <td>ornel</td>\n",
       "      <td>usretireehealthcareplan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALGN</th>\n",
       "      <td>invisalign</td>\n",
       "      <td>scanner</td>\n",
       "      <td>orthodontist</td>\n",
       "      <td>dental</td>\n",
       "      <td>orthoclear</td>\n",
       "      <td>ormco</td>\n",
       "      <td>orthodont</td>\n",
       "      <td>intraor</td>\n",
       "      <td>patient</td>\n",
       "      <td>itero</td>\n",
       "      <td>...</td>\n",
       "      <td>prescott</td>\n",
       "      <td>clinic</td>\n",
       "      <td>scan</td>\n",
       "      <td>brace</td>\n",
       "      <td>teen</td>\n",
       "      <td>clearanc</td>\n",
       "      <td>cadcam</td>\n",
       "      <td>juarez</td>\n",
       "      <td>dentist</td>\n",
       "      <td>msu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EW</th>\n",
       "      <td>lifesci</td>\n",
       "      <td>edward</td>\n",
       "      <td>valv</td>\n",
       "      <td>heart</td>\n",
       "      <td>transcathet</td>\n",
       "      <td>sapien</td>\n",
       "      <td>medtron</td>\n",
       "      <td>perfus</td>\n",
       "      <td>aortic</td>\n",
       "      <td>clinic</td>\n",
       "      <td>...</td>\n",
       "      <td>lifest</td>\n",
       "      <td>corevalv</td>\n",
       "      <td>cardiac</td>\n",
       "      <td>surgic</td>\n",
       "      <td>diseas</td>\n",
       "      <td>perimount</td>\n",
       "      <td>hemodynam</td>\n",
       "      <td>contentsedward</td>\n",
       "      <td>unconsolid</td>\n",
       "      <td>carpentieredward</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   word_0          word_1          word_2          word_3  \\\n",
       "tickers                                                                     \n",
       "AMZN            amazoncom            peac          amazon            ship   \n",
       "BBY                   sga       musicland      merchandis        applianc   \n",
       "BKNG                hotel    pricelinecom      bookingcom          airlin   \n",
       "MCD               restaur     companyoper        mcdonald       franchise   \n",
       "EBAY               paypal            ebay          seller           skype   \n",
       "F                    ford         automot           motor      incomeloss   \n",
       "HD                  depot      merchandis              hd             sga   \n",
       "TGT                  card           guest  comparablestor         redcard   \n",
       "WHR             whirlpool          befiex       brazilian         indesit   \n",
       "JPM              jpmorgan      lendingrel         securit            card   \n",
       "SIVB                  svb          client     noninterest         silicon   \n",
       "CFG              incmanag           basel             frb            cbna   \n",
       "C               citigroup         securit            card           basel   \n",
       "ALL               allstat         reinsur      propertyli         annuiti   \n",
       "IVZ                   aum         invesco             clo             cip   \n",
       "ETFC                etrad      fourfamili        brokerag           ajaxo   \n",
       "MET                metlif      policyhold         reinsur         annuiti   \n",
       "PFG            policyhold         annuiti             pfg       residenti   \n",
       "CBOE                 cboe             bat             occ            opra   \n",
       "CTL           centurylink           qwest          embarq      centurytel   \n",
       "IPG             interpubl          client             cmg             ipg   \n",
       "NFLX                  dvd        subscrib          stream         netflix   \n",
       "CHTR                 cabl             cco          holdco             cch   \n",
       "FB               facebook             mau             dau      zuckerberg   \n",
       "TWTR              twitter           tweet             mau            mdau   \n",
       "NWSA                  fox            news             rea          foxtel   \n",
       "FOXA                  fox           sport         televis       broadcast   \n",
       "AMD                   amd             fab              gf  microprocessor   \n",
       "INTC                intel  microprocessor         chipset       processor   \n",
       "AAPL                 ipod            appl             mac           iphon   \n",
       "LRCX                  lam   semiconductor            etch           wafer   \n",
       "MSFT            microsoft          window            xbox          server   \n",
       "CTSH               cogniz          indian          client            rupe   \n",
       "ADS                  card            mile             wfn         securit   \n",
       "WU       consumertoconsum      companynot         spinoff           subag   \n",
       "ABT                abbott      laboratori      pharmaceut          nutrit   \n",
       "CVS              pharmaci        caremark              cv            drug   \n",
       "PFE                pfizer      pharmaceut             fda           wyeth   \n",
       "JNJ               johnson         janssen      pharmaceut           depuy   \n",
       "BIIB              rituxan         tysabri          biogen            idec   \n",
       "INCY                 drug          jakafi          clinic        collabor   \n",
       "HSIC               schein           henri          dental      practition   \n",
       "WAT                 water              ta            hplc  chromatographi   \n",
       "ALGN           invisalign         scanner    orthodontist          dental   \n",
       "EW                lifesci          edward            valv           heart   \n",
       "\n",
       "                        word_4          word_5          word_6  \\\n",
       "tickers                                                          \n",
       "AMZN                    seller            bezo    equitymethod   \n",
       "BBY                   magnolia           squad            geek   \n",
       "BKNG                    ticket       hotelscom           kayak   \n",
       "MCD                   franchis           apmea       systemwid   \n",
       "EBAY                       gsi        merchant          ticket   \n",
       "F                      securit           volvo           mazda   \n",
       "HD                        expo            card          lumber   \n",
       "TGT                 merchandis             sga            ebit   \n",
       "WHR                     maytag         embraco        applianc   \n",
       "JPM                  chargeoff             var             msr   \n",
       "SIVB               contentssvb          valley       nonmarket   \n",
       "CFG                         rb            alll             var   \n",
       "C                          gcb            tier             vie   \n",
       "ALL               contracthold         reestim             dac   \n",
       "IVZ                     client             uit            seed   \n",
       "ETFC                       tdr    mortgageback    interestearn   \n",
       "MET                       mlic             dac  contentsmetlif   \n",
       "PFG                        aum              bt     realizedunr   \n",
       "CBOE                       cfe              fx         euroccp   \n",
       "CTL                  broadband             fcc           savvi   \n",
       "IPG                        ian      motorsport          mccann   \n",
       "NFLX                   librari       subscript      membership   \n",
       "CHTR                       twc           video        franchis   \n",
       "FB                        arpu        whatsapp           oculu   \n",
       "TWTR                      spam             ipa         audienc   \n",
       "NWSA                   centuri             nam         newspap   \n",
       "FOXA                   station            cabl          disney   \n",
       "AMD                   spansion           wafer         graphic   \n",
       "INTC                      imft          memori           flash   \n",
       "AAPL                 macintosh            itun              os   \n",
       "LRCX                   fremont         novellu        shipment   \n",
       "MSFT                       oem              pc    headcountrel   \n",
       "CTSH                        im       fixedpric            visa   \n",
       "ADS                     ebitda           comen          reward   \n",
       "WU                    speedpay  consumertobusi           wufsi   \n",
       "ABT                        tap        diagnost        vascular   \n",
       "CVS                        pbm       prescript         medicar   \n",
       "PFE                    lipitor            drug       pharmacia   \n",
       "JNJ      jnjpharmaceuticalmemb           cordi             jpi   \n",
       "BIIB                    avonex       genentech        collabor   \n",
       "INCY                     incyt             fda          candid   \n",
       "HSIC                      anim       officebas       physician   \n",
       "WAT                         lc         thermal    spectrometri   \n",
       "ALGN                orthoclear           ormco       orthodont   \n",
       "EW                 transcathet          sapien         medtron   \n",
       "\n",
       "                           word_7                       word_8  \\\n",
       "tickers                                                          \n",
       "AMZN                   merchandis                     ecommerc   \n",
       "BBY                          shop                           ar   \n",
       "BKNG                          car                     merchant   \n",
       "MCD                   development                         menu   \n",
       "EBAY                         card                      stubhub   \n",
       "F                          dealer                        truck   \n",
       "HD                         assort                        carol   \n",
       "TGT                       scovann                      remodel   \n",
       "WHR                        sundri                   compressor   \n",
       "JPM                   noninterest                          pci   \n",
       "SIVB                     bancshar                       unfund   \n",
       "CFG                        incnot                  noninterest   \n",
       "C                        citicorp                          cdo   \n",
       "ALL                    catastroph                          aic   \n",
       "IVZ                          csip                   policyhold   \n",
       "ETFC                    chargeoff                         fhlb   \n",
       "MET                  metropolitan                         voba   \n",
       "PFG                  mortgageback                investmenttyp   \n",
       "CBOE                          vix                  contentscbo   \n",
       "CTL                           lec                     wireless   \n",
       "IPG                           pip              continuedamount   \n",
       "NFLX                         movi                       studio   \n",
       "CHTR                        allen                       vulcan   \n",
       "FB                      instagram                        monet   \n",
       "TWTR                    instagram                 personnelrel   \n",
       "NWSA                 harpercollin                      valassi   \n",
       "FOXA     foxtelevisionsegmentmemb  usgaapoperatingsegmentsmemb   \n",
       "AMD                         intel                      chipset   \n",
       "INTC                          dcg                    nonmarket   \n",
       "AAPL                        music                         ipad   \n",
       "LRCX                     warranti                           rd   \n",
       "MSFT                       consol                         game   \n",
       "CTSH                      offshor                          stp   \n",
       "ADS                       epsilon                          air   \n",
       "WU                     contentsth                    crosscurr   \n",
       "ABT                         abbvi                         jude   \n",
       "CVS                     retailltc                      dispens   \n",
       "PFE                       medicin                      patient   \n",
       "JNJ                          anda                          jbi   \n",
       "BIIB                       clinic                      zevalin   \n",
       "INCY                  ruxolitinib                      patient   \n",
       "HSIC                   pharmaceut                         drug   \n",
       "WAT                          uplc                      acquiti   \n",
       "ALGN                      intraor                      patient   \n",
       "EW                         perfus                       aortic   \n",
       "\n",
       "                  word_9  ...         word_20                  word_21  \\\n",
       "tickers                   ...                                            \n",
       "AMZN                  aw  ...        shipment               wwwamazond   \n",
       "BBY              ebitdar  ...        tradenam                    phone   \n",
       "BKNG        pricedisclos  ...         expedia            rentalcarscom   \n",
       "MCD           refranchis  ...              nm                    japan   \n",
       "EBAY               rolex  ...     counterfeit                processor   \n",
       "F                    fce  ...            veba                   jaguar   \n",
       "HD                omnibu  ...             rdc                  remodel   \n",
       "TGT                 jpmc  ...              ep              supertarget   \n",
       "WHR             pricemix  ...        warranti                 refriger   \n",
       "JPM               client  ...              nm                     lend   \n",
       "SIVB        equityventur  ...             fte               riskweight   \n",
       "CFG                 tier  ...             msr                   noncor   \n",
       "C                conduit  ...             cva                       fx   \n",
       "ALL              homeown  ...      policyhold                      pif   \n",
       "IVZ               passiv  ...         domicil                    sterl   \n",
       "ETFC                 cmo  ...           sweep                     lien   \n",
       "MET              asbesto  ...    contracthold               agricultur   \n",
       "PFG                 dpac  ...            iowa                 swaption   \n",
       "CBOE                 etp  ...            cftc                      spx   \n",
       "CTL                  usf  ...            voip                  verizon   \n",
       "IPG               dooner  ...             fcb        reorganizationrel   \n",
       "NFLX                gato  ...            mail                     hast   \n",
       "CHTR                  cc  ...          bright          subsidiariesnot   \n",
       "FB               messeng  ...             dap                 unproven   \n",
       "TWTR               monet  ...   similarlytitl                  retweet   \n",
       "NWSA           australia  ...           paytv                  televis   \n",
       "FOXA              ebitda  ...         credibl                  caffein   \n",
       "AMD               saxoni  ...           micro                processor   \n",
       "INTC               mcafe  ...         desktop           corporationnot   \n",
       "AAPL            warranti  ...  microprocessor                      app   \n",
       "LRCX               clean  ...       klatencor                 outsourc   \n",
       "MSFT                 msn  ...         voucher                entertain   \n",
       "CTSH       onsiteoffshor  ...          coburn                     wage   \n",
       "ADS         brandloyalti  ...        merchant                  databas   \n",
       "WU       westernunioncom  ...            card                antimoney   \n",
       "ABT                 aler  ...             amo                   solvay   \n",
       "CVS                aetna  ...           payor                formulari   \n",
       "PFE               lyrica  ...   biopharmaceut  pfebiopharmasegmentmemb   \n",
       "JNJ              ethicon  ...        diagnost          usgaapnonusmemb   \n",
       "BIIB            copromot  ...        mileston                 antibodi   \n",
       "INCY            mileston  ...       inhibitor                   pfizer   \n",
       "HSIC             valuead  ...             fda                  bergman   \n",
       "WAT            rheometri  ...   uspensionplan         nonuspensionplan   \n",
       "ALGN               itero  ...        prescott                   clinic   \n",
       "EW                clinic  ...          lifest                 corevalv   \n",
       "\n",
       "                      word_22                                   word_23  \\\n",
       "tickers                                                                   \n",
       "AMZN            wwwamazoncouk                                    unearn   \n",
       "BBY                   remodel                                  notebook   \n",
       "BKNG                 braddock                                  schulman   \n",
       "MCD                   chicken                                   convent   \n",
       "EBAY                   client                                 copyright   \n",
       "F                         pag                                      fuel   \n",
       "HD                      ferri                                     blake   \n",
       "TGT                      visa                                       trc   \n",
       "WHR               forwardsopt                                    oilrel   \n",
       "JPM                        af                              creditimpair   \n",
       "SIVB                  standbi                                      tier   \n",
       "CFG                      lend                                       lcr   \n",
       "C                         tob                                 residenti   \n",
       "ALL                    mortal                               lifeconting   \n",
       "IVZ                unconsolid                                fundoffund   \n",
       "ETFC            contentsetrad                                nonperform   \n",
       "MET                    surplu                                      cede   \n",
       "PFG           contentsprincip                                 postretir   \n",
       "CBOE             exchangetrad                              multiplylist   \n",
       "CTL                     video                                      cabl   \n",
       "IPG                  uncommit                                worldgroup   \n",
       "NFLX                    video                                      reed   \n",
       "CHTR                     viii                                      pole   \n",
       "FB                     wehner                                   younger   \n",
       "TWTR                    coloc                                 tellapart   \n",
       "NWSA               betterwors                                 broadcast   \n",
       "FOXA                      fcc  foxredeemablenoncontrollinginterestsmemb   \n",
       "AMD                        jv                                   fujitsu   \n",
       "INTC                 debentur                                   pentium   \n",
       "AAPL                  nontrad                                     video   \n",
       "LRCX                   varian                                    bullen   \n",
       "MSFT                subscript                                    search   \n",
       "CTSH                    unbil                                       mat   \n",
       "ADS                     score                                   conduit   \n",
       "WU       continuedunauditedth                                    mexico   \n",
       "ABT                       fda                                  scientif   \n",
       "CVS               cvspharmaci                                       pdp   \n",
       "PFE                        eh                                        rd   \n",
       "JNJ                     lupin                                biosimilar   \n",
       "BIIB                  samsung                                 fumapharm   \n",
       "INCY                calithera                              myelofibrosi   \n",
       "HSIC                patterson                                laboratori   \n",
       "WAT                 micromass                                   yeartod   \n",
       "ALGN                     scan                                     brace   \n",
       "EW                    cardiac                                    surgic   \n",
       "\n",
       "                 word_24           word_25  \\\n",
       "tickers                                      \n",
       "AMZN       wwwamazoncojp             pledg   \n",
       "BBY            speakeasi         entertain   \n",
       "BKNG            pricelin               nca   \n",
       "MCD               nutrit         breakfast   \n",
       "EBAY            meritori              shop   \n",
       "F          statementsnot         nonconsum   \n",
       "HD                menear           atlanta   \n",
       "TGT              marshal         minnesota   \n",
       "WHR                monet              alno   \n",
       "JPM             multisel              tier   \n",
       "SIVB             riskrat           volcker   \n",
       "CFG           nonperform          nonaccru   \n",
       "C                    htm                af   \n",
       "ALL                 fhcf               lbl   \n",
       "IVZ             subgroup             starr   \n",
       "ETFC             ltvcltv        riskweight   \n",
       "MET                 auto               rmb   \n",
       "PFG               demutu  residentialfirst   \n",
       "CBOE          unrestrict            outcri   \n",
       "CTL        facilitiesbas             rural   \n",
       "IPG              philipp            ebitda   \n",
       "NFLX     internetconnect          suboptim   \n",
       "CHTR             comcast           televis   \n",
       "FB                nasdaq        inccondens   \n",
       "TWTR             traffic             googl   \n",
       "NWSA           newsprint            sunday   \n",
       "FOXA                 lot              game   \n",
       "AMD        semiconductor               wsa   \n",
       "INTC            notebook        intergraph   \n",
       "AAPL                 rsu          outsourc   \n",
       "LRCX               tegal          newberri   \n",
       "MSFT            datacent            surfac   \n",
       "CTSH              export               dun   \n",
       "ADS           chargedoff          montreal   \n",
       "WU                 ersek            biller   \n",
       "ABT               dental             xienc   \n",
       "CVS           pharmacist        merchandis   \n",
       "PFE                 pain               mda   \n",
       "JNJ             actelion             asrtm   \n",
       "BIIB              biolog           therapi   \n",
       "INCY            arthriti           license   \n",
       "HSIC               benco            surgic   \n",
       "WAT                  lcm            academ   \n",
       "ALGN                teen          clearanc   \n",
       "EW                diseas         perimount   \n",
       "\n",
       "                                          word_26  \\\n",
       "tickers                                             \n",
       "AMZN                                      jeffrey   \n",
       "BBY                                       televis   \n",
       "BKNG                                        ctrip   \n",
       "MCD                                        omnibu   \n",
       "EBAY                                  butterfield   \n",
       "F                                        warranti   \n",
       "HD                                         mexico   \n",
       "TGT                                     periodend   \n",
       "WHR                                      hotpoint   \n",
       "JPM                                           spe   \n",
       "SIVB                                     debentur   \n",
       "CFG                                           occ   \n",
       "C                                             var   \n",
       "ALL                                      casualti   \n",
       "IVZ                                lossesreinvest   \n",
       "ETFC                                     nonaccru   \n",
       "MET                                          lend   \n",
       "PFG                                        mortal   \n",
       "CBOE                                      chicago   \n",
       "CTL                                       interst   \n",
       "IPG                                   passthrough   \n",
       "NFLX                                          vod   \n",
       "CHTR                                         cchc   \n",
       "FB                                            bug   \n",
       "TWTR                                       ebitda   \n",
       "NWSA                                          sun   \n",
       "FOXA                                          nfl   \n",
       "AMD                                          atmp   \n",
       "INTC                                semiconductor   \n",
       "AAPL                                     cellular   \n",
       "LRCX                   usgaapforwardcontractsmemb   \n",
       "MSFT                                     intellig   \n",
       "CTSH                                   bradstreet   \n",
       "ADS                                     indexalli   \n",
       "WU                                      crossbord   \n",
       "ABT                                       freyman   \n",
       "CVS      usgaapfairvaluemeasurementsrecurringmemb   \n",
       "PFE                                       norvasc   \n",
       "JNJ                                        jersey   \n",
       "BIIB                                    neurimmun   \n",
       "INCY                                        maxia   \n",
       "HSIC                                       clinic   \n",
       "WAT                                     quadrupol   \n",
       "ALGN                                       cadcam   \n",
       "EW                                      hemodynam   \n",
       "\n",
       "                                word_27                word_28  \\\n",
       "tickers                                                          \n",
       "AMZN                            longzon                wrongdo   \n",
       "BBY                            canadian               auctionr   \n",
       "BKNG                           agodacom                  googl   \n",
       "MCD                              commod                  occup   \n",
       "EBAY                          billpoint                launder   \n",
       "F                                commod              assetback   \n",
       "HD                                 kpmg             contentsth   \n",
       "TGT                              dougla                   gift   \n",
       "WHR                               amana             kitchenaid   \n",
       "JPM                                  cb             heldforsal   \n",
       "SIVB                         bancventur                    ftp   \n",
       "CFG                 investmentpostmodif      contractspremodif   \n",
       "C                             prioryear                  cgmhi   \n",
       "ALL                                 cmb      creditriskconting   \n",
       "IVZ                            amvescap                     ep   \n",
       "ETFC                               fdic                   otti   \n",
       "MET                            unitlink                     af   \n",
       "PFG                              cuprum                 mortar   \n",
       "CBOE                               cbsx                    sef   \n",
       "CTL         usgaapoperatingsegmentsmemb                    mpl   \n",
       "IPG                      mccannerickson              prioryear   \n",
       "NFLX                          freetrial               mccarthi   \n",
       "CHTR                             settop               programm   \n",
       "FB                               server                    app   \n",
       "TWTR                           inaccess                   fake   \n",
       "NWSA                          harlequin               subscrib   \n",
       "FOXA                            footbal  usgaapcommonstockmemb   \n",
       "AMD                               flash                   atic   \n",
       "INTC                              wafer            motherboard   \n",
       "AAPL                            carrier              powerbook   \n",
       "LRCX                         yendenomin                   espp   \n",
       "MSFT                             commod         corporatelevel   \n",
       "CTSH                            holiday                     ub   \n",
       "ADS                               aspen                breakag   \n",
       "WU                            gainsloss                arizona   \n",
       "ABT                      cardiovascular           endovascular   \n",
       "CVS                                 qui           silverscript   \n",
       "PFE                              vaccin                     ih   \n",
       "JNJ                                 fda               lifescan   \n",
       "BIIB                          therapeut             hemophilia   \n",
       "INCY                             cancer              codevelop   \n",
       "HSIC     hsichealthcaredistributionmemb                 vaccin   \n",
       "WAT                                  pe                  ornel   \n",
       "ALGN                             juarez                dentist   \n",
       "EW                       contentsedward             unconsolid   \n",
       "\n",
       "                         word_29  \n",
       "tickers                           \n",
       "AMZN                       kindl  \n",
       "BBY                      auction  \n",
       "BKNG                      walker  \n",
       "MCD                          eat  \n",
       "EBAY                    licensur  \n",
       "F                            ghg  \n",
       "HD                         floor  \n",
       "TGT                      linkbas  \n",
       "WHR                          raw  \n",
       "JPM                      conduit  \n",
       "SIVB                mortgageback  \n",
       "CFG                      appetit  \n",
       "C                   mortgageback  \n",
       "ALL                  equityindex  \n",
       "IVZ                     canadian  \n",
       "ETFC                        etbh  \n",
       "MET                          cse  \n",
       "PFG                       bifurc  \n",
       "CBOE                        chix  \n",
       "CTL                     wholesal  \n",
       "IPG                       carrol  \n",
       "NFLX               latticebinomi  \n",
       "CHTR                         cii  \n",
       "FB                    inaccuraci  \n",
       "TWTR                      server  \n",
       "NWSA                       ebook  \n",
       "FOXA                    subscrib  \n",
       "AMD                         amtc  \n",
       "INTC                    wireless  \n",
       "AAPL                     desktop  \n",
       "LRCX                      pariba  \n",
       "MSFT                       skype  \n",
       "CTSH                      scienc  \n",
       "ADS                         wfcb  \n",
       "WU                      subpoena  \n",
       "ABT                          cfr  \n",
       "CVS                        linen  \n",
       "PFE                       disord  \n",
       "JNJ                     franchis  \n",
       "BIIB                        roch  \n",
       "INCY                        gene  \n",
       "HSIC                      archer  \n",
       "WAT      usretireehealthcareplan  \n",
       "ALGN                         msu  \n",
       "EW              carpentieredward  \n",
       "\n",
       "[45 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "df_top_k_word.to_csv(\"data/45_companies_top_30_stemming.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "with open(os.path.join(pkl_path, 'token_counter.pkl'), 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    c = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyze 10K word with tfidf and bag-of-words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "\"\"\"\n",
    "This part is to compute the tf values for the words in 10ks and collect the overall word list for computing idf in the next step\n",
    "\"\"\"\n",
    "#count the number of 10K documents we have\n",
    "document_num_10k = 0\n",
    "\n",
    "#word list for 10k \n",
    "word_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        #we have document for a given date, so add 1 for document_num_10k\n",
    "        document_num_10k += 1\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        all_data[ticker]['10ks'][date]['tf'] = Counter(all_data[ticker]['10ks'][date]['words'].split())\n",
    "        \n",
    "        #iterate through the words in tf, which is the words of a given 10k document of a given date\n",
    "        for word in all_data[ticker]['10ks'][date]['tf']:\n",
    "            \n",
    "            #add one if it already contains the word, or add the this word to the dict if not\n",
    "            word_list_10k[word] += 1\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#compute idf value for the word in 10ks\n",
    "idf_10k = {}\n",
    "\n",
    "#iterate through all the words in word_list_10k\n",
    "for word in word_list_10k:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10k[word] = np.log(document_num_10k / (1 + word_list_10k[word]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10k files\n",
    "# document_num_10k = document_num_10k\n",
    "\n",
    "#word pair list for 10k\n",
    "pair_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        #there is no need for counting document_num_10k\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10ks'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "#         #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10ks'][date]['tf_pair'] = df_temp.to_dict()[0]\n",
    "#         print(all_data[ticker]['10ks'][date]['tf_pair'])\n",
    "        #print(all_data[ticker]['10ks'][date]['tf_pair'])\n",
    "        #iterate through the pairs in tf_pair, which is the words of a given 10k document of a given date\n",
    "        for pair in all_data[ticker]['10ks'][date]['tf_pair']:\n",
    "            \n",
    "            #add one if it already contains the pair, or add the this pair to the dict if not have\n",
    "            pair_list_10k[pair] += 1\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "pair_list_10k"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the pair in 10ks\n",
    "idf_10k_pair = {}\n",
    "\n",
    "#iterate through all the pairs in pair_list_10k\n",
    "for pair in pair_list_10k:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10k_pair[pair] = np.log(document_num_10k / (1 + pair_list_10k[pair]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10k files\n",
    "# document_num_10k = document_num_10k\n",
    "\n",
    "#word triple list for 10k\n",
    "triple_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        #there is no need for counting document_num_10k\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (3, 3))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10ks'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10ks'][date]['tf_triple'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        #iterate through the triples in tf_triple, which is the words of a given 10k document of a given date\n",
    "        for triple in all_data[ticker]['10ks'][date]['tf_triple']:\n",
    "            \n",
    "            #add one if it already contains the triple, or add the this triple to the dict if not have\n",
    "            triple_list_10k[triple] += 1\n",
    "            \n",
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the triple in 10ks\n",
    "idf_10k_triple = {}\n",
    "\n",
    "#iterate through all the triples in triple_list_10k\n",
    "for triple in triple_list_10k:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10k_triple[triple] = np.log(document_num_10k / (1 + triple_list_10k[triple]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### doing the same to 10qs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "#also do the same to 10q files\n",
    "\n",
    "\"\"\"\n",
    "This part is to compute the tf values for the words in 10qs and collect the overall word list for computing idf in the next step\n",
    "\"\"\"\n",
    "#count the number of 10Q documents we have\n",
    "document_num_10q = 0\n",
    "\n",
    "#word list for 10q \n",
    "word_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        #we have document for a given date, so add 1 for document_num_10q\n",
    "        document_num_10q += 1\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        all_data[ticker]['10qs'][date]['tf'] = Counter(all_data[ticker]['10qs'][date]['words'].split())\n",
    "        \n",
    "        #iterate through the words in tf, which is the words of a given 10q document of a given date\n",
    "        for word in all_data[ticker]['10qs'][date]['tf']:\n",
    "            \n",
    "            #add one if it already contains the word, or add the this word to the dict if not\n",
    "            word_list_10q[word] += 1\n",
    "               "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "#compute idf value for the word in 10qs\n",
    "idf_10q = {}\n",
    "\n",
    "#iterate through all the words in word_list_10q\n",
    "for word in word_list_10q:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10q[word] = np.log(document_num_10q / (1 + word_list_10q[word]))\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10q files\n",
    "# document_num_10q = document_num_10q\n",
    "\n",
    "#word pair list for 10q\n",
    "pair_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        #there is no need for counting document_num_10q\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10qs'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10qs'][date]['tf_pair'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        #iterate through the pairs in tf_pair, which is the words of a given 10q document of a given date\n",
    "        for pair in all_data[ticker]['10qs'][date]['tf_pair']:\n",
    "            \n",
    "            #add one if it already contains the pair, or add the this pair to the dict if not have\n",
    "            pair_list_10q[pair] += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the pair in 10qs\n",
    "idf_10q_pair = {}\n",
    "\n",
    "#iterate through all the pairs in pair_list_10q\n",
    "for pair in pair_list_10q:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10q_pair[pair] = np.log(document_num_10q / (1 + pair_list_10q[pair]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10q files\n",
    "# document_num_10q = document_num_10q\n",
    "\n",
    "#word triple list for 10q\n",
    "triple_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        #there is no need for counting document_num_10q\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        \"\"\"\n",
    "        The only adaptation from pair to triple is changing ngram_range, needing futher simplification of code\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (3, 3))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10qs'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10qs'][date]['tf_triple'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        #iterate through the triples in tf_triple, which is the words of a given 10q document of a given date\n",
    "        for triple in all_data[ticker]['10qs'][date]['tf_triple']:\n",
    "            \n",
    "            #add one if it already contains the triple, or add the this triple to the dict if not have\n",
    "            triple_list_10q[triple] += 1\n",
    "            \n",
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the triple in 10qs\n",
    "idf_10q_triple = {}\n",
    "\n",
    "#iterate through all the triples in triple_list_10q\n",
    "for triple in triple_list_10q:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10q_triple[triple] = np.log(document_num_10q / (1 + triple_list_10q[triple]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### have a look at the data structure"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "idf_10k"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "\"\"\"\n",
    "This is new in version 2, containing idfs for word pairs\n",
    "\"\"\"\n",
    "\n",
    "idf_10q_pair"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "\"\"\"\n",
    "This is new in version 2, containing idfs for word pairs\n",
    "\"\"\"\n",
    "\n",
    "idf_10q_triple"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Store the data for future use "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "\"\"\"\n",
    "this part is for storing the data for future use\n",
    "\"\"\"\n",
    "#delete word in all_data for storage\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        del all_data[ticker]['10qs'][date]['words']\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        del all_data[ticker]['10ks'][date]['words']\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "#write all_data to a json file\n",
    "with open('all_data.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(all_data))\n",
    "\n",
    "#write idf_10k to a json file\n",
    "with open('idf_10k.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k))\n",
    "\n",
    "#write idf_10q to a json file    \n",
    "with open('idf_10q.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q))\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "\"\"\"\n",
    "This is new in version 2, storing files for word pairs and word triples\n",
    "\"\"\"\n",
    "#write idf_10k_pair to a json file  \n",
    "with open('idf_10k_pair.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k_pair))\n",
    "\n",
    "#write idf_10q_pair to a json file    \n",
    "with open('idf_10q_pair.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q_pair))\n",
    "    \n",
    "#write idf_10k_triple to a json file  \n",
    "with open('idf_10k_triple.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k_triple))\n",
    "\n",
    "#write idf_10q_triple to a json file    \n",
    "with open('idf_10q_triple.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q_triple))    \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "interpreter": {
   "hash": "b893d7568335c74a8df3e912797c1bbf5cf11a7925c935f70126c10891afeb03"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}